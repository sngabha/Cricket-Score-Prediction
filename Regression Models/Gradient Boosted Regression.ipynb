{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os.path\n",
    "import math\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-computed stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle(os.path.abspath('') + '\\\\..\\\\Derived stats\\\\match_stats_train.pkl')\n",
    "df_test = pd.read_pickle(os.path.abspath('') + '\\\\..\\\\Derived stats\\\\match_stats_test.pkl')\n",
    "\n",
    "# delete outlier\n",
    "\"\"\" Assuming, in most normal matches, teams \n",
    "score more than 75 runs on an average as shown\n",
    "by past statistics. \"\"\"\n",
    "df_train = df_train[df_train['Runs'] > 75]\n",
    "df_test = df_test[df_test['Runs'] > 75]\n",
    "\n",
    "cols = df_train.columns\n",
    "cols = cols.map(lambda x: x.replace(' ', '_') if isinstance(x, (str, 'utf-8')) else x)\n",
    "df_train.columns = cols\n",
    "df_test.columns = cols\n",
    "\n",
    "# To remove the other inning for which outliers were removed\n",
    "match_ids = df_test['Match_ID'].unique()\n",
    "m_ids_toBeRemoved = []          \n",
    "for m_id in match_ids:\n",
    "    if len(df_test[df_test['Match_ID'] == m_id]) !=2:\n",
    "        m_ids_toBeRemoved.append(m_id)\n",
    "\n",
    "for m_id in m_ids_toBeRemoved:\n",
    "    df_test = df_test[df_test['Match_ID'] != m_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing additional features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding feature: Average runs scored against same opponent in previous matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Previous run with same opponent \"\"\"\n",
    "\n",
    "prev_run = []\n",
    "for Date, Team_Name, Opp_Team  in df_train[['Date','Team_Name', 'Opp_Team']].itertuples(index=False):\n",
    "    df_t = df_train.query(\"Team_Name == @Team_Name & Opp_Team == @Opp_Team & Date < @Date\")['Runs']\n",
    "    if len(df_t) == 0:\n",
    "        prev_run.append(df_train.query(\"Team_Name == @Team_Name & Opp_Team == @Opp_Team\")['Runs'].mean())\n",
    "        continue\n",
    "    prev_run.append(df_train.query(\"Team_Name == @Team_Name & Opp_Team == @Opp_Team & Date < @Date\")['Runs'].values[-1])\n",
    "df_train['Prev Run'] = prev_run\n",
    "\n",
    "prev_run = []\n",
    "for Date, Team_Name, Opp_Team  in df_test[['Date','Team_Name', 'Opp_Team']].itertuples(index=False):\n",
    "    df_t = df_test.query(\"Team_Name == @Team_Name & Opp_Team == @Opp_Team & Date < @Date\")['Runs']\n",
    "    if len(df_t) == 0:\n",
    "        df_t_t = df_train.query(\"Team_Name == @Team_Name & Opp_Team == @Opp_Team & Date < @Date\")\n",
    "        if len(df_t_t) == 0:\n",
    "            prev_run.append(0)\n",
    "        else:\n",
    "            prev_run.append(df_t_t['Runs'].values[-1])\n",
    "        continue\n",
    "    prev_run.append(df_test.query(\"Team_Name == @Team_Name & Opp_Team == @Opp_Team & Date < @Date\")['Runs'].values[-1])\n",
    "df_test['Prev Run'] = prev_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding feature: Average runs scored in a venue till date irrespective of teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Previous run on venue irrespective of team \"\"\"\n",
    "\n",
    "prev_run = []\n",
    "for Date, city  in df_train[['Date', 'City']].itertuples(index=False):\n",
    "    df_t = df_train.query(\"City == @city & Date < @Date\")\n",
    "    if len(df_t) == 0:\n",
    "        df_t_t = df_train.query(\"City == @city\")\n",
    "        if len(df_t_t) == 0:\n",
    "            prev_run.append(0)\n",
    "        else:\n",
    "            prev_run.append(df_t_t['Runs'].mean())\n",
    "        continue\n",
    "    prev_run.append(df_t['Runs'].mean())\n",
    "df_train['Prev Venue Run'] = prev_run\n",
    "\n",
    "prev_run = []\n",
    "for Date, city  in df_test[['Date', 'City']].itertuples(index=False):\n",
    "    df_te = df_test.query(\"City == @city & Date < @Date\")\n",
    "    df_tr = df_train.query(\"City == @city & Date < @Date\")\n",
    "    prev_run.append((df_te['Runs'].sum(axis = 0, skipna = True) + df_tr['Runs'].sum(axis = 0, skipna = True))/(len(df_te)+len(df_tr)))\n",
    "df_test['Prev Venue Run'] = prev_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print data before model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Features to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_inning_1 = df_train[df_train['Innnings'] == 1]\n",
    "df_test_inning_1 = df_test[df_test['Innnings'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inning_2 = df_test[df_test['Innnings'] == 2]\n",
    "inning_2_input = df_inning_2[['Avg_Bat1', 'Avg_Bat2', 'Avg_Bat3', 'Avg_Bat4', 'Avg_Bat5', 'Avg_Bat6', 'Avg_Bat7', 'Avg_Bat8', 'Avg_Bat9', 'Avg_Bat10', 'Avg_Bat11','Prev Run', 'Prev Venue Run', 'Past_lead']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Avg_Bat1', 'Avg_Bat2', 'Avg_Bat3', 'Avg_Bat4', 'Avg_Bat5', 'Avg_Bat6', 'Avg_Bat7', 'Avg_Bat8', 'Avg_Bat9', 'Avg_Bat10', 'Avg_Bat11', 'Prev Run', 'Prev Venue Run', 'Past_lead']\n",
    "\n",
    "X_train = df_train_inning_1[cols]\n",
    "y_train = df_train_inning_1[['Runs']]\n",
    "\n",
    "X_test = df_test_inning_1[cols]\n",
    "y_test = df_test_inning_1[['Runs']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=GradientBoostingRegressor(alpha=0.9,\n",
       "                                                 criterion='friedman_mse',\n",
       "                                                 init=None, learning_rate=0.1,\n",
       "                                                 loss='ls', max_depth=3,\n",
       "                                                 max_features=None,\n",
       "                                                 max_leaf_nodes=None,\n",
       "                                                 min_impurity_decrease=0.0,\n",
       "                                                 min_impurity_split=None,\n",
       "                                                 min_samples_leaf=1,\n",
       "                                                 min_samples_split=2,\n",
       "                                                 min_weight_fraction_leaf=0.0,\n",
       "                                                 n_estimators=100,\n",
       "                                                 n_iter...\n",
       "                                                 validation_fraction=0.1,\n",
       "                                                 verbose=0, warm_start=False),\n",
       "             iid=False, n_jobs=-1,\n",
       "             param_grid={'learning_rate': array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ]),\n",
       "                         'max_depth': [2, 3, 4],\n",
       "                         'n_estimators': array([ 10,  20,  30,  40,  50,  60,  70,  80,  90, 100, 110, 120, 130,\n",
       "       140, 150, 160, 170, 180, 190, 200])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dict = {'max_depth': [2,3,4], 'learning_rate': np.linspace(0.01,0.1,10), 'n_estimators': np.linspace(10,200,num = 20, dtype = int)}\n",
    "gradientRegressionModel = GradientBoostingRegressor()\n",
    "gradientRegressionModel_cv = GridSearchCV(estimator=gradientRegressionModel, param_grid=param_dict, cv= 5, iid=False, n_jobs = -1)\n",
    "gradientRegressionModel_cv.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train random regression model RMSE: 21.395961899457035\n"
     ]
    }
   ],
   "source": [
    "predictedRunsTrain = gradientRegressionModel_cv.predict(X_train)\n",
    "print('Train random regression model RMSE:', np.sqrt(mean_squared_error(y_train, predictedRunsTrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test random regression Linear model RMSE: 22.711199367815382\n"
     ]
    }
   ],
   "source": [
    "predictedRunsTest = gradientRegressionModel_cv.predict(X_test)\n",
    "print('Test random regression Linear model RMSE:', np.sqrt(mean_squared_error(y_test, predictedRunsTest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "                          learning_rate=0.06000000000000001, loss='ls',\n",
       "                          max_depth=2, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=60,\n",
       "                          n_iter_no_change=None, presort='auto',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradientRegressionModel_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of winner prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = pd.read_csv('matches.csv')\n",
    "matchId_winnerInning_dict = {}\n",
    "for index, row in matches.iterrows():\n",
    "    if row['winner'] == row['team1']:\n",
    "        matchId_winnerInning_dict[row['id']] = 1\n",
    "    if row['winner'] == row['team2']:\n",
    "        matchId_winnerInning_dict[row['id']] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.77777777777777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohinish\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "inning1_out = predictedRunsTest\n",
    "inning2_out = gradientRegressionModel_cv.predict(inning_2_input)\n",
    "\n",
    "winner_df = df_inning_2[['Match_ID']]\n",
    "winner = [ 1 if inning1_out[i] > inning2_out[i] else 2 for i in range(len(inning1_out))]\n",
    "winner_df['Winner'] = winner\n",
    "correct_count = 0\n",
    "for match_id, winner  in winner_df[['Match_ID', 'Winner']].itertuples(index=False):\n",
    "    if winner == matchId_winnerInning_dict[match_id]:\n",
    "        correct_count += 1\n",
    "winPredictionAccuracy = float((correct_count*100)/len(winner_df))\n",
    "\n",
    "print(winPredictionAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Scores prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot and see the data..\n",
    "for id, runs in df_test[['Match_ID', 'Runs']].itertuples(index=False):\n",
    "    print(runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for runs in predictedRunsTest:\n",
    "    print(runs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
